{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssetReplacementEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AssetReplacementEnv, self).__init__()\n",
    "\n",
    "        self.n_actions = 3\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "\n",
    "        # With spaces.Tuple you can create a multidimensional state\n",
    "        # In our simple model observation and state are synonyms\n",
    "        self.n_states = 6\n",
    "        self.observation_space = spaces.Discrete(self.n_states)\n",
    "\n",
    "        self.n_maint = 6\n",
    "        self.maint_space = spaces.Discrete(self.n_maint)\n",
    "        \n",
    "        self.service = 10\n",
    "        self.cost = 75\n",
    "        self.rng = default_rng()\n",
    "   \n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        State transition of the model.\n",
    "\n",
    "        \"\"\"\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n",
    "\n",
    " \n",
    "    \n",
    "        # calculate the reward and the state transition\n",
    "        if action == 2:                                    \n",
    "            reward = self.profit(0) - self.cost         # replace\n",
    "            self.state = 1 \n",
    "            self.maint = 0\n",
    "            \n",
    "        elif action == 1:\n",
    "            reward = self.profit(self.state) - self.service\n",
    "            self.state += 1\n",
    "            self.maint += 1 \n",
    "               \n",
    "                \n",
    "        else: \n",
    "            reward = self.profit(self.state) \n",
    "            self.state += 1    \n",
    "\n",
    "        return self.state, self.maint, reward\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def profit(self, state):\n",
    "        return 50 - 2.5*state - 2.5*state**2\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        # set initial state (age of machine) to 0\n",
    "        self.state = 0\n",
    "        self.maint = 0            \n",
    "        return self.state, self.maint\n",
    "\n",
    "\n",
    "    # We will not implement render and close function\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "    def close (self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "    def __init__(self, agent_info):\n",
    "        \"\"\"Initialize Agent\n",
    "\n",
    "        Args: \n",
    "            agent_info (dict): Parameters used to initialize agent.\n",
    "            {\n",
    "                n_actions (int): Number of actions.\n",
    "                epsilon (float): Exploration parameter.\n",
    "                step_size (float): Learning rate alpha.\n",
    "                discount (float): Discount factor gamma.\n",
    "            }\n",
    "        \"\"\"\n",
    "        self.n_actions = agent_info[\"n_actions\"]\n",
    "        self.epsilon = agent_info[\"epsilon\"]\n",
    "        self.step_size = agent_info[\"step_size\"]\n",
    "        self.discount = agent_info[\"discount\"]\n",
    "\n",
    "        self.rng = default_rng()\n",
    "\n",
    "        # Create an array for action-value estimates and initialize it to zero.\n",
    "        self.q = defaultdict(lambda: np.zeros(self.n_actions))      \n",
    "\n",
    "\n",
    "    def step(self, reward, state, maint):\n",
    "        \"\"\"A step taken by the agent\n",
    "\n",
    "        Args:\n",
    "            next_state (int):  next state from the environment\n",
    "            next_maint (int):  next state of maintenance failure from environment \n",
    "        Returns:\n",
    "            next_action (int): action the agent takes in next_state\n",
    "        \"\"\"\n",
    "\n",
    "        # choose action using epsilon greedy policy\n",
    "        action = self.select_action(state, maint)\n",
    "\n",
    "            \n",
    "        target = reward + self.discount*np.max(self.q[state, maint])   \n",
    "        td_error = target - self.q[self.previous_state,self.previous_maint][self.previous_action]\n",
    "        self.q[self.previous_state,self.previous_maint][self.previous_action] += self.step_size* td_error\n",
    "\n",
    "\n",
    "        # save current state and action\n",
    "        self.previous_state = state\n",
    "        self.previous_maint = maint\n",
    "        self.previous_action = action\n",
    "\n",
    "        return action\n",
    "\n",
    "        \n",
    "    def select_action(self, state, maint):\n",
    "        \"\"\"Select action using epsilon greedy policy\n",
    "\n",
    "        Args:\n",
    "            state (int): current state\n",
    "            maint (int): current times of maitenance failure\n",
    "        Returns:\n",
    "            action (int): action the agent takes\n",
    "        \"\"\"\n",
    "        \n",
    "        # performe epsilon greedy policy improvement\n",
    "        # remember to replace asset if age is 5\n",
    "        if state== 5:\n",
    "            return 2\n",
    "        \n",
    "        if  self.rng.random() < self.epsilon:\n",
    "            action = self.rng.choice(env.n_actions)\n",
    "        else:\n",
    "            action = self.argmax(self.q[state, maint])\n",
    "        return action\n",
    "\n",
    "\n",
    "    def argmax(self, q_values):\n",
    "        \"\"\"Return the index of maximum value with ties broken randomly.\n",
    "\n",
    "        Args:\n",
    "            q_values (numpy.ndarray): A shape-(n_actions,) array of estimated\n",
    "                action_values.\n",
    "\n",
    "        Returns:\n",
    "            index (int): Index of the maximal value.\n",
    "        \"\"\"\n",
    "\n",
    "        ties = np.flatnonzero(np.isclose(q_values, max(q_values)))\n",
    "        index = self.rng.choice(ties)\n",
    "\n",
    "        return index\n",
    "\n",
    "\n",
    "    def start(self, state, maint):\n",
    "        \"\"\"Selects action in inital state\n",
    "\n",
    "        Args:\n",
    "            state (int): initial state\n",
    "            maint (int): initial maint\n",
    "        Returns:\n",
    "            action (int): initial action\n",
    "        \"\"\"\n",
    "\n",
    "        action = 0\n",
    "\n",
    "        self.previous_maint= maint\n",
    "        self.previous_state = state\n",
    "        self.previous_action = action\n",
    "\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = AssetReplacementEnv()\n",
    "\n",
    "epsilon = 0.3\n",
    "step_size = 0.2\n",
    "discount = 0.9\n",
    "\n",
    "\n",
    "agent_info = {\"n_actions\": env.n_actions,\n",
    "              \"epsilon\": epsilon,\n",
    "              \"step_size\": step_size,\n",
    "              \"discount\": discount}\n",
    "\n",
    "\n",
    "agent = QLearningAgent(agent_info)\n",
    "\n",
    "n_episodes = 10000\n",
    "timesteps = 20\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    state, maint = env.reset()\n",
    "    action = agent.start(state, maint)\n",
    "\n",
    "    for t in range(timesteps):\n",
    "        agent.epsilon = 1/timesteps+ 0.001\n",
    "        mu = (state - maint)/5\n",
    "        state, maint, reward, = env.step(action)\n",
    "        if action == 2:\n",
    "            pass\n",
    "        elif action ==1:\n",
    "            if env.rng.random() < mu:\n",
    "                reward = 0 - env.service\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            if env.rng.random() < mu:\n",
    "                reward = 0\n",
    "            else:\n",
    "                pass     \n",
    "        action = agent.step(reward, state, maint)\n",
    "\n",
    "V = defaultdict(float)\n",
    "policy = defaultdict(int)\n",
    "for state_maint, values in sorted(agent.q.items()):\n",
    "    value = np.max(values)\n",
    "    V[state_maint] = value\n",
    "    action = np.argmax(values)\n",
    "    policy[state_maint] = action\n",
    "\n",
    "\n",
    "\n",
    "#convert dictionary to array so that we can compare the norm of different value function\n",
    "V_QL = np.zeros((env.n_states, env.n_maint))\n",
    "for k, v in V.items():\n",
    "    V_QL[k[0],k[1]] = v\n",
    "norm = np.linalg.norm(V_QL-V_DP)\n",
    "print('Norm of difference of value function at episodes'+ str(n_episodes)+' simulation:' + str(norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
